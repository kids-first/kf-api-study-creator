############################################################
# Generates a mock study for testing ingestion on the
# DataService and loads it into a local DataService
# container.
############################################################

import base32_crockford as b32
import os
import pandas as pd
import random as ra
from urllib.parse import urlparse

from d3b_utils.aws_bucket_contents import fetch_aws_bucket_obj_info

def kf_id_generator(prefix):
    """
    Returns a function to generator
    (Crockford)[http://www.crockford.com/wrmg/base32.html] base 32
    encoded number up to 8 characters left padded with 0 and prefixed with
    a two character value representing the entity type and delimited by
    an underscore
    Ex:
    'PT_0004PEDE'
    'SA_D167JSHP'
    'DM_ZZZZZZZZ'
    'ST_00000000'
    """
    assert len(prefix) == 2, 'Prefix must be two characters'
    prefix = prefix.upper()

    def generator():
        return '{0}_{1:0>8}'.format(prefix,
                                    b32.encode(ra.randint(0, 32**8-1)))

    return generator


def create_part_kf_ids(N=10):
    part_gen = kf_id_generator("PT")
    return [part_gen() for _ in range(N)]


def create_bio_manifest(parts, N=10):
    """
    Create a tabular data file representing a Biospecimen manifest. This
    just includes the necessary attributes to ingest Biospecimen entities.
    N rows are generated, one for each Biospecimen. The number needed is
    up to the user. The simplest solution is to have one per participant.
    _parts_ is a list of the KF-IDs of the participants, generated by the
    _create_part_kf_ids_ function.
    """
    bio_gen = kf_id_generator("BS")
    _range = range(N)
    bio_dict = {
        "kf_id": [bio_gen() for _ in _range],
        "sample_id": [f"SM-{i}" for i in _range],
        "participant_id": [f"CARE-{i}" for i in _range],
        "part_kf_id": parts,
        "gender": [ra.choice(("Female", "Male")) for _ in _range],
        "volume": [100] * N,
        "concentration": [30] * N,
        "family_id": [ra.choice(("FA-1", "FA-2")) for _ in _range],
        "tissue_type": [ra.choice(("Blood", "Saliva")) for _ in _range],
    }

    pd.DataFrame(bio_dict).to_csv(
        "data/bio_manifest.csv",
        index=False
    )


def create_gen_manifest(parts):
    """
    Create a tabular data file representing a GenomicFile manifest.
    _parts_ is the same as above.
    """
    N = 10
    _range = range(N)
    gen_gen = kf_id_generator("GF")
    _full_range = range(N*3)
    S3_PREFIX = (
        "s3://kf-study-us-east-1-prd-sd-me0wme0w/source/genomic-files/"
    )
    exts = {".cram", ".cram.crai", ".cram.md5"}
    gen_dict = {
        "kf_id": [gen_gen() for _ in _full_range],
        "project_id": ["RP-1" for _ in _full_range],
        "biospec_id": [f"SM-{i}" for i in _range for _ in range(3)],
        "data_type": ["WGS" for _ in _full_range],
        "participant_id": [f"CARE-{i}" for i in _range for _ in range(3)],
        "part_kf_id": [part for part in parts for _ in range(3)],
        "filepath": [
            f"{S3_PREFIX}genomic-file-{i}{ext}" for i in _range for ext in exts
        ],
    }
    pd.DataFrame(gen_dict).to_csv(
        "data/gen_manifest.csv",
        index=False
    )


def create_s3_scrape():
    """
    Create a tabular data file representing an S3 scrape of metadata for the
    genomic files (both unharmonized and harmonized).
    """
    buckets = {
       "s3://kf-study-us-east-1-prd-sd-me0wme0w/source/genomic-files/",
       "s3://kf-study-us-east-1-prd-sd-me0wme0w/harmonized/simple-variants/"
    }
    s3_df = scrape_s3(buckets)
    s3_df.to_csv(
        "data/s3_scrape.csv",
        index=False,
    )


def scrape_s3(buckets):
    """
    Perform the scrape for each bucket in _buckets_. Return a DataFrame _df_
    which has all this data.
    """
    dfs = []
    for bucket in buckets:
        parsed_s3 = urlparse(bucket)
        bucket_df = pd.DataFrame(
            fetch_aws_bucket_obj_info(
                bucket_name=parsed_s3.netloc,
                search_prefixes=parsed_s3.path,
                drop_folders=True,
            )
        )
        bucket_df["Filepath"] = f"s3://{parsed_s3.netloc}/" + bucket_df["Key"]
        dfs.append(bucket_df)

    df = pd.concat(dfs)
    df["Filename"] = df["Key"].apply(lambda x: os.path.split(x)[-1])
    return df


if __name__ == "__main__":
    ra.seed(0)
    parts = create_part_kf_ids()
    create_bio_manifest(parts)
    create_gen_manifest(parts)
    create_s3_scrape()
